################################################################################

### 2-compile_exsitu_data.R

### Author: Emily Beckman Bruns
### Funding: Base script was funded by the Institute of Museum and Library 
# Services (IMLS MFA program grant MA-30-18-0273-18 to The Morton Arboretum).
# Moderate edits were added with funding from a cooperative agreement
# between the United States Botanic Garden and San Diego Botanic Garden
# (subcontracted to The Morton Arboretum), with support from
# Botanic Gardens Conservation International U.S.

### Creation date: 13 December 2019
### Last updated: 05 December 2022
### R version 4.2.2

### DESCRIPTION:
# This script takes a folder of CSV files representing accessions data from
# different botanical garden collections, combines them into one dataset, 
# and standardizes some important fields.
# !Please note! that significant user input and edits are needed, since every 
# ex situ dataset is unique. Go slowly step-by-step and continue checking that 
# everything is as expected. 

### INPUTS:
#
# 1. exsitu_standard_column_names (folder)
#    Accessions data from ex situ collections survey; CSV files whose column
#    names have been standardized by hand using instructions in the
#    "Processing ex situ data" tab in Gap-analysis-workflow_metadata.xlsx
#
# 2. institution_data_table.csv
#    Table with metadata for institutions who provided accessions data during
#    the ex situ collections survey; see example in
#    Gap-analysis-workflow_metadata.xlsx
#
# 3. target_taxa_with_synonyms.csv
#    List of target taxa and synonyms; see example in
#    Gap-analysis-workflow_metadata.xlsx
#
# 4. Accession-level data downloads from international crop genebank
#    databases; download instructions are provided within the script below
# (a) Genesys [Global Crop Diversity Trust]:
#     https://www.genesys-pgr.org/a/overview
# (b) WIEWS [FAO's World information and early warning system on plant genetic
#     resources for food and agriculture]:
#     https://www.fao.org/wiews/data/ex-situ-sdg-251/search/en/?no_cache=1
#
# 5. Polygons...

### OUTPUTS:
# 

################################################################################
# Load libraries
################################################################################

my.packages <- c('plyr', 'tidyverse', 'textclean', 'spatialEco',
                 'maps', 'measurements', 'CoordinateCleaner', 'raster',
                 'data.table', 'terra', 'tidyterra')
# install.packages (my.packages) #Turn on to install current versions
lapply(my.packages, require, character.only=TRUE)
rm(my.packages)

select <- dplyr::select
rename <- dplyr::rename
filter <- dplyr::filter
group_by <- dplyr::group_by
mutate <- dplyr::mutate
distinct <- dplyr::distinct

################################################################################
# Set working directory
################################################################################

# either set manually:
#main_dir <- "/Volumes/GoogleDrive-103729429307302508433/My Drive/CWR North America Gap Analysis/Gap-Analysis-Mapping"

# or use 0-set_working_directory.R script:
source("/Users/emily/Documents/GitHub/SDBG_CWR-trees-gap-analysis/0-set_working_directory.R")

# set folders where you have raw data (in) and want processed data to go (out)
exsitu_dir <- paste0(main_dir,"/ex-situ_data")
data_in <- "exsitu_standard_column_names"
data_out <- "OUTPUTS_FROM_R"

################################################################################
# Load functions
################################################################################

# function to read in ex situ files from different folders/years and stack
read.exsitu.csv <- function(path,submission_year){
  # create list of paths to ex situ accessions CSV files in folder
  file_list <- list.files(path=path,pattern=".csv",full.names=TRUE)
  # read in each csv in path list to create list of dataframes
  file_dfs <- lapply(file_list,read.csv,header=TRUE,fileEncoding="LATIN1",
    strip.white=TRUE,colClasses="character",na.strings=c("","NA"))
  print(paste0("Number of files: ",length(file_dfs)))
    #sapply(file_dfs, nrow) # can look at number of rows in each csv
  for(file in seq_along(file_dfs)){
    df <- file_dfs[[file]]
    # add file name as column, to record home institution for each record
    df$filename <- rep(file_list[file],nrow(df))
    # remove file path portion
    df$filename <- mgsub(df$filename,c(paste0(path,"/"),".csv"),"")
    # add year of submission
    df$submission_year <- submission_year
    # remove extra blank columns
    t <- grepl("^X",names(df))
    if(length(unique(t))>1){
      #print(df$filename[1])
      df <- df[, -grep("^X", names(df))]
    }
    # replace strange characters in column names (arise from saving?)
    names(df) <- gsub(x = names(df), pattern = "ï\\.\\.", replacement = "")
    # add accession number if there isn't one
    if("acc_num" %in% names(df) & nrow(df[which(is.na(df$acc_num)),]) > 0){
      df[which(is.na(df$acc_num)),]$acc_num <- paste0("added",
        sprintf("%04d", 1:nrow(df[which(is.na(df$acc_num)),])))
      #print(nrow(df))
    } else if ("acc_no" %in% names(df) & nrow(df[which(is.na(df$acc_no)),]) > 0){
      df[which(is.na(df$acc_no)),]$acc_no <- paste0("added",
        sprintf("%04d", 1:nrow(df[which(is.na(df$acc_no)),])))
      #print(nrow(df))
    } else if (!("acc_num" %in% names(df)) & !("acc_no" %in% names(df))){
      df$acc_num <- paste0("added", sprintf("%04d", 1:nrow(df)))
      #print(nrow(df))
    } else {
      #print(paste("NO ACC NUM EDITS:",df$filename[1]))
    }
    # replace old df with new df
    file_dfs[[file]] <- df
    #print(head(file_dfs[[file]],n=2))
  }
  # stack all datasets using rbind.fill, which keeps non-matching columns
  #   and fills with NA; 'Reduce' iterates through and merges with previous
  # this may take a few minutes if you have lots of data
  all_data <- Reduce(rbind.fill, file_dfs)
    print(paste0("Number of rows: ",nrow(all_data)))
    print(paste0("Number of columns: ",ncol(all_data)))
  return(all_data)
}

################################################################################
# 1. Read in and stack all ex situ accessions data from survey of collections
################################################################################

### FIRST: After receiving accession data from institutions, you need to process
#   it manually. See here for instructions:
#   https://docs.google.com/spreadsheets/d/1p5HAS7vIE-3CbQcUmwrnuBGv5324dXy-42Iu6LlbX0E/edit?usp=sharing

# read in data from multiple surveys and stack, or just read in from one folder.
# this function also adds columns for 1) the file name [often equivalent to the
# "inst_short" institution nickname] 2) a submission year, 3) an accession
# number if one isn't given
### CHANGE BASED ON FOLDER(S) AND YEAR(S) YOU HAVE
all_data <- read.exsitu.csv(file.path(exsitu_dir,data_in), "2022") #168
# stack all data if you had multiple years:
#to_stack <- list(raw_2022,raw_2021,raw_2020,raw_2019,raw_2018,raw_2017)
#all_data <- Reduce(rbind.fill, to_stack)

### IF APPLICABLE:
sort(colnames(all_data)) # check for duplicates/typos
  # combine multiple inst_short columns
#all_data <- tidyr::unite(all_data,"inst_short",
#  c("inst_short","inst_short2"),
#  sep=";",remove=T,na.rm=T)
  # combine multiple taxon name columns
  # we'll do the rest of the column standardizing later; this is to get genus
all_data <- tidyr::unite(all_data,"taxon_full_name",
  c("taxon_full_name","taxon_name_full","taxon.full_name"),
  sep=";",remove=T,na.rm=T)

# fill in inst_short column with filename if none provided
all_data$inst_short[is.na(all_data$inst_short)] <-
  all_data$filename[is.na(all_data$inst_short)]
# remove rows with no inst_short
all_data <- all_data[which(!is.na(all_data$inst_short)),]
all_data <- all_data[which(all_data$inst_short!="ManualEntry"),]

# read in institution metadata file, for comparing list to data read in
inst_data <- read.csv(file.path(exsitu_dir,
  "respondent_institution_data_table.csv"), stringsAsFactors = F)
#inst_data$parent_R_file <- gsub("\\{PARENT\\}","",inst_data$parent_R_file)
## CHECK ALL INSTITUTIONS ARE PRESENT
#sort(unique(all_data$inst_short)) #333
#sort(unique(inst_data$inst_short)) #340
  ### INSTITUTIONS IN THE DATA BUT NOT IN METADATA TABLE
  # this should be "character(0)"
unique(all_data$inst_short)[!(unique(all_data$inst_short) %in% 
                                unique(inst_data$inst_short))]
  ### INSTITUTIONS IN METADATA TABLE BUT NOT IN THE DATA:
  # this should just be parent files (have mult institutions),
  # for example, PCNQuercus
unique(inst_data$inst_short)[!(unique(inst_data$inst_short) %in% 
                               unique(all_data$inst_short))]

# fill genus column so we can see which datasets have our target genus
  # fill genus column if not already filled
all_data <- all_data %>% 
  separate("taxon_full_name","genus_temp",sep=" ",remove=F)
all_data[which(is.na(all_data$genus)),]$genus <- 
  all_data[which(is.na(all_data$genus)),]$genus_temp
  # standardize capitalization
all_data$genus <- str_to_title(all_data$genus)
  ### FIX GENUS MISSPELLINGS OR ABBREVIATIONS
sort(unique(all_data$genus))
#all_data$genus <- mgsub(all_data$genus,
#  c("^Q$","^Q\\.$","Querces","Quercues","Querucs","Querus","Qurercus","Cyclobalanopsis"),
#  "Quercus",fixed=F)
#all_data$taxon_full_name <- mgsub(all_data$taxon_full_name,
#  c("Q\\.","Cyclobalanopsis"), "Quercus",fixed=F)

# remove duplicate data - from previous years or networks
### UPDATE THIS SECTION BASED ON YOUR SPECIFIC NEEDS; THIS IS FOR QUERCUS 2022:
nrow(all_data) #25653
#all_data$file_inst_year <- paste(all_data$filename,all_data$inst_short,all_data$submission_year)
#  # 1) remove datasets that have no Quercus data (so we know they aren't dups)
#inst_genera <- all_data %>%
#  group_by(filename,inst_short,submission_year) %>%
#  mutate(genera = paste(unique(genus),collapse = ", ")) %>%
#  ungroup() %>% select(inst_short,filename,submission_year,genera)
#yes_Q <- unique(inst_genera[which(grepl("Quercus",inst_genera$genera)),]) %>%
#  mutate(file_inst_year = paste(filename,inst_short,submission_year))
#all_data <- all_data %>% filter(file_inst_year %in% yes_Q$file_inst_year)
#nrow(all_data) #156714
#  # 2) remove old datasets if there is newer data from 2018-2020
#all_data$inst_and_year <- paste(all_data$inst_short,all_data$submission_year)
#find_dups <- all_data %>%
#  filter(submission_year != "2021" & submission_year != "2022") %>%
#  group_by(inst_short,submission_year) %>%
#  count() %>% ungroup() %>%
#  arrange(desc(submission_year)); find_dups
#remove_dups <- find_dups[which(duplicated(find_dups$inst_short)),]
#remove_dups <- paste(remove_dups$inst_short,remove_dups$submission_year); remove_dups
#all_data <- all_data %>% filter(!(inst_and_year %in% remove_dups))
#nrow(all_data) #127495
#  # 3) if there is duplicate network data, keep only the data direct from garden
#network_data <- all_data %>% filter(inst_short != filename &
#  (submission_year != "2022" & submission_year != "2021"))
#own_data <- all_data %>% filter(inst_short == filename &
#  (submission_year != "2022" & submission_year != "2021"))
#remove <- sort(unique(network_data[which(
#  network_data$inst_short %in% own_data$inst_short),]$file_inst_year)); remove
#all_data <- all_data %>%
#  filter(!(file_inst_year %in% remove))
#nrow(all_data) #126189

# check out column names
  #not_all_na <- function(x) any(!is.na(x))
  #all_data <- all_data %>% select(where(not_all_na))
sort(colnames(all_data))
## IF NEEDED: separate column into multiple
#all_data <- all_data %>% separate("specific",
#  c("infra_rank_add","infra_name_add"),sep=" ",remove=T,fill="right")
## IF NEEDED: see which datasets have extraneous columns so you can fix manually
##  in the raw data as desired; update line below
#unique(all_data$filename[all_data$locality.1 !=""])
## IF NEEDED: merge similar columns (you may not need to do this if no schema
##  mistakes were made when manually editing column names)
all_data <- tidyr::unite(all_data,"cultivar",
                         c("cultivar","cultivsr"),
                         sep="; ",remove=T,na.rm=T)
all_data <- tidyr::unite(all_data,"genus", 
                         c("genus","gensu"),
                         sep="; ",remove=T,na.rm=T)
all_data <- tidyr::unite(all_data,"hybrid", 
                         c("hybrid","hyrbid"),
                         sep="; ",remove=T,na.rm=T)
all_data <- tidyr::unite(all_data,"notes", 
                         c("notes","Notes","notes2","inst_short2"),
                         sep="; ",remove=T,na.rm=T)
all_data <- tidyr::unite(all_data,"taxon_verif", 
                         c("taxon_verif","Taxonomic.Verification","taxon_det"),
                         sep="; ",remove=T,na.rm=T)
all_data <- tidyr::unite(all_data,"coll_year", 
                         c("coll_year","coll_date","rec_year","planted_year","rec_date"),
                         sep="; ",remove=T,na.rm=T)
## IF NEEDED: rename columns
#all_data <- all_data %>% rename(gps_det = coord_notes)
## IF NEEDED: change # of individuals for rows that say dead/removed, etc.
sort(unique(all_data$condition))
#all_data[which(all_data$condition=="Dead"),]$num_individuals <- "0"

## REMOVE UNUSED COLUMNS (SELECT WHAT YOU WANT TO KEEP)
sort(colnames(all_data))
keep_col <- c("acc_num","assoc_sp","author","coll_name","coll_num",
  "coll_year","country","county","cultivar","filename",
  "garden_loc","genus","germ_type","hybrid","infra_name",
  "infra_rank","inst_short","lin_num","locality","municipality","notes",
  "num_indiv","orig_lat","orig_long","orig_source","prov_type","rec_as",
  "species","state","submission_year","taxon_full_name","taxon_verif")
all_data <- all_data[,keep_col]

# remove leading, trailing, and middle (e.g., double space) whitespace,
#   to prevent future errors
all_data <- as.data.frame(lapply(all_data, function(x) str_squish(x)),
  stringsAsFactors=F)
# replace "" cells with NA in whole dataset
all_data[all_data == ""] <- NA
# add data source colunm
all_data$data_source <- "ex_situ_BG_survey"
# first fix some common lat/long character issues before replacing non-ascii
all_data$orig_lat <- mgsub(all_data$orig_lat,
  c("\"","ç","d","'","°","º","Â","¼","،","¡","_","´","*","À","?","`")," ")
all_data$orig_long <- mgsub(all_data$orig_long,
  c("\"","ç","d","'","°","º","Â","¼","،","¡","_","´","*","À","?","`")," ")
# now replace all non-ascii characters
all_data <- as.data.frame(lapply(all_data,replace_non_ascii),stringsAsFactors=F)

################################################################################
# 2. Compile Genesys data (genebanks)
################################################################################

### FIRST, download data from Genesys [Global Crop Diversity Trust]:
# Go to https://www.genesys-pgr.org/a/overview
# Search for target genus/genera (in the TAXONOMY tab on the left) then click
# "APPLY FILTERS" at the top left.
# Click the "ACCESSIONS" tab in the top bar, then click "DOWNLOAD ZIP" on the
# right.
# Move the downloaded folder to your ex situ working directory (exsitu_dir) and
# rename to "genesys-accessions"
# You can view field metadata here: https://www.genesys-pgr.org/documentation/basics

# load in data
  # collection site description
genPath <- paste0(exsitu_dir,"/genesys-accessions/coll.csv")
  # I think the "Found and resolved improper quoting" warning is ok
gen_col <- data.table::fread(file = genPath, header = TRUE)
  str(gen_col)
  nrow(gen_col) ; length(unique(gen_col$genesysId)) #333
  # metadata about record
genPath <- paste0(exsitu_dir,"/genesys-accessions/core.csv")
gen_core <- data.table::fread(genPath, header = TRUE)
  str(gen_core)
  nrow(gen_core) ; length(unique(gen_core$genesysId)) #428
  # spatial info
  #   for some reason the headers dont read in correctly with fread;
  #   will use read.csv just to get the headers and add them to the fread df
genPath <- paste0(exsitu_dir,"/genesys-accessions/geo.csv")
gen_geo <- data.table::fread(genPath, header = TRUE)
  str(gen_geo)
  colnames(gen_geo)
gen_geo_head <- read.csv(genPath)
  colnames(gen_geo_head)
gen_geo <- gen_geo %>% select(-V8) # remove the last column (nothing in it)
colnames(gen_geo) <- colnames(gen_geo_head)
  str(gen_geo)
  nrow(gen_geo) ; length(unique(gen_geo$genesysId)) #427

# combine all dataframes
genesys <- Reduce(full_join,list(gen_col,gen_core,gen_geo)) #,gen_names
str(genesys); nrow(genesys)
# create taxon name column
## We don't need to do this here since we work with all taxon names below!
  # fix up infra rank/name column a little
#sort(unique(genesys$subtaxa))
#genesys$subtaxa <- mgsub(genesys$subtaxa,
#  c("\\(l.\\) dun.","^s$","~sp","^a$","^i$","^m$"),"",fixed=F)
#genesys$subtaxa <- mgsub(genesys$subtaxa,c("var.","subsp."),c("var. ","subsp. "))
#genesys$subtaxa <- str_squish(genesys$subtaxa)
#sort(unique(genesys$subtaxa))
  # create full name
#genesys$taxon_full_name <- paste(genesys$genus,genesys$species,genesys$subtaxa)
#genesys$taxon_name <- str_squish(genesys$taxon_name)
  # replace hybrid x with +
#genesys$taxon_name <- gsub(" x "," +",genesys$taxon_name)
#sort(unique(genesys$taxon_name))
  # add 'Genesys' to ID
genesys$UID <- paste0("Genesys-",genesys$genesysId)
  # paste some similar columns together
genesys <- tidyr::unite(genesys,"coll_year",
  c("collDate","acqDate"),sep=";",remove=T,na.rm=T)

# rename columns to match ex situ data and select only those we need
genesys_sel <- genesys %>%
  rename(taxon_full_name = fullTaxa,
         coll_num = collNumb,
         locality = collSite,
         notes = uuid,
         inst_short = instCode,
         acc_num = acceNumb,
         infra_rank = subtaxa,
         country = origCty,
         orig_lat = latitude,
         orig_long = longitude,
         gps_det = method,
         germ_type = storage,
         orig_source = collSrc,
         prov_type = sampStat) %>%
  select(UID,taxon_full_name,coll_num,coll_year,locality,notes,
    inst_short,acc_num,infra_rank,country,orig_lat,orig_long,gps_det,
    germ_type,orig_source,prov_type,genus,species,uncertainty)

# filter by target taxa & add taxon data
## We do this later!
#genesys_sel <- left_join(genesys_sel,taxon_list)
#genesys_target <- genesys_sel %>% filter(!is.na(taxon_name_acc))
#nrow(genesys_target)

# add data source colun
genesys_sel$data_source <- "Genesys"

# join to exsitu data
all_data <- rbind.fill(all_data,genesys_sel)
nrow(all_data) #104085

## YOU EITHER NEED TO REMOVE GRIN OR REMOVE US INSTITUTIONS IN GENSYS:
## To remove US Genesys:
# US institutions covered by GRIN (I think); we will remove these
# pulled this list from CWR Gap Analysis 2020 (Khoury et al.)...
USDAcodes <- c("USA003" ,"USA004", "USA005" ,"USA016" ,"USA020",
"USA022", "USA026", "USA028", "USA029", "USA042" ,"USA047", "USA049",
"USA074", "USA108", "USA133", "USA148", "USA151", "USA167", "USA176",
"USA390", "USA955", "USA956", "USA970", "USA971", "USA995")
all_data <- all_data %>% filter(!(inst_short %in% USDAcodes))
## To remove GRIN:
#all_data <- all_data %>% filter(inst_short != "GRIN_NPGS")

nrow(all_data) #89588

################################################################################
# 3. Compile WIEWS data (genebanks)
################################################################################

# 23 Nov 2022: For some reason the data aren't downloading; skipping this
# section for Quercus (Genesys gets most of the genebanks anyways).

### FIRST, download data from WIEWS [FAO's World information and early warning
# system on plant genetic resources for food and agriculture]:
# Go to https://www.fao.org/wiews/data/ex-situ-sdg-251/search/en/?no_cache=1
# In the "Crop Wild Relatives" dropdown, select "Included".
# Type your target genus into the "Genus" box and press the "+" button on the
# right. Add additional target genera like this as needed.
# Scroll down and press the magnifying glass icon on the bottom right.
# Now click the "Download Results" button on the top right.
# Move the downloaded file to your ex situ working directory (exsitu_dir) and
# rename to "wiews-exsitu.csv"

# read in data
wiewsPath <- paste0(exsitu_dir,"/Wiews_Exsitu.csv")
#wiews <- data.table::fread(wiewsPath, header = TRUE)
wiews <- read.csv(wiewsPath)
str(wiews) #95551

# filter out Genesys data
wiews <- wiews %>%
  filter(Source.of.information != "Genesys (https://www.genesys-pgr.org)")
nrow(wiews) #81051

# create taxon name column
## We don't need to do this here since we work with all taxon names below!
  # Spilt name to get at genus and species
#wiews$name <- wiews$Taxon
#wiews <- tidyr::separate(data = wiews, "name",
#  into =c('genus','spec','sub1','sub2','sub3', 'sub4'),sep=' ')
#  # from "wiewsTransform.R" by Dan Carver:
#  #   Function to split full name into taxon/species
#setSpecies <- function(dataFrame){
#  if(!is.na(dataFrame$sub1)){
#    dataFrame$species <- paste(dataFrame$spec,dataFrame$sub1,
#                               dataFrame$sub2, sep="_")
#  }
#  if(is.na(dataFrame$sub1)){
#    dataFrame$species <- dataFrame$spec
#  }
#  return(dataFrame)
#}
#wiews2 <- setSpecies(wiews)
# remove all NA
#df6 <- str_remove_all(df4$species, 'NA') %>%
#  str_remove_all("__")
#df4$species <- df6
#df4 <- subset(x = df4, select = -c(spec,sub1,sub2) )

# rename columns to match ex situ data and select only those we need
wiews_sel <- wiews %>%
  rename(inst_short = Holding.institute.code,
         acc_num = Accession.number,
         taxon_full_name = Taxon,
         genus = Genus,
         species = Species,
         rec_date = Acquisition.date..YYYY.MM.,
         country = Country.of.origin..ISO3.,
         prov_type = Biological.status,
         notes = Genebank.s..holding.safety.duplications...code,
         orig_lat = Latitude.of.collecting.site..decimal.degrees.format.,
         orig_long = Longitude.of.collecting.site..decimal.degrees.format.,
         orig_source = Collecting.acquisition.source,
         germ_type = Type.of.germplasm.storage) %>%
  select(inst_short,acc_num,taxon_full_name,genus,species,rec_date,country,
         prov_type,notes,orig_lat,orig_long,orig_source,germ_type)

# !!! JUST FOR NOW; need to check exact to filter out !!!
#   US institutions (covered by GRIN?)
# looks like they are all removed by removing Genesys anyways, so we'll skip
#wiews_target <- wiews_sel %>% filter(!grepl("USA",inst_short))
nrow(wiews_sel) #81051

# add data source colun
wiews_sel$data_source <- "FAO-WIEWS"

# join to exsitu data
all_data <- rbind.fill(all_data,wiews_sel)
nrow(all_data) #170639

################################################################################
# 4. Save raw output of compiled data for target genera
#     (can be used to look for hybrids/cultivars, which are removed in next step)
################################################################################

# create folder for output data
if(!dir.exists(file.path(exsitu_dir,data_out)))
  dir.create(file.path(exsitu_dir,data_out), recursive=T)

all_data2 <- all_data
nrow(all_data2) #126267

# read in target taxa list
taxon_list <- read.csv(file.path(main_dir, "target_taxa_with_synonyms.csv"),
  header = T, na.strings = c("","NA"),colClasses = "character")
str(taxon_list)
#unique(taxon_list$taxon_region)
  # can select just a specific region if you want
#taxon_list <- taxon_list %>%
#  filter(taxon_region == "Mesoamerican" |
#         taxon_region == "Mesoamerican & US" |
#         taxon_region == "US")
nrow(taxon_list) #287

# preserve original taxon name
all_data2$taxon_full_name_orig <- all_data2$taxon_full_name

# remove rows not in target genus/genera
  # can either create list manually...
#target_genera <- c("Quercus")
  # or pull from target taxa list if you have genus separated...
target_genera <- unique(taxon_list$genus)
# filter by genus/genera
all_data2 <- all_data2 %>% filter(genus %in% target_genera)
nrow(all_data); nrow(all_data2) #170639 ; 157662

### CHECK OUT THE HYBRID COLUMN ###
sort(unique(all_data2$hybrid))
# standardize so only one hybrid symbol is used ( x )
all_data2$hybrid <- mgsub(all_data2$hybrid,
  c(" A ","^A ","^A$"," X ","^X"," _ ","^_ ","^_$","^1$","\\*","^H$","^hyb$","Hybrid"),
  " x ", fixed=F)
# add "x" at beginning then remove duplicates
all_data2$hybrid[!is.na(all_data2$hybrid)] <- paste0("x ",all_data2$hybrid[!is.na(all_data2$hybrid)])
all_data2$hybrid <- str_squish(all_data2$hybrid)
#all_data2$hybrid <- str_to_lower(all_data2$hybrid)
all_data2$hybrid <- mgsub(all_data2$hybrid,c("x x","x h$"),"x",fixed=F)
sort(unique(all_data2$hybrid))

# create concatenated taxon_full_name column
all_data2 <- tidyr::unite(all_data2, "taxon_full_name_concat",
  c(genus,hybrid,species,infra_rank,infra_name,cultivar), sep=" ", remove=F,
  na.rm=T)

# when blank, fill taxon_full_name column with concatenated full name
all_data2[is.na(all_data2$taxon_full_name),]$taxon_full_name <-
  all_data2[is.na(all_data2$taxon_full_name),]$taxon_full_name_concat
#unique(all_data2$taxon_full_name)

# standardize common hybrid signifiers in taxon_full_name
all_data2$taxon_full_name <- mgsub(all_data2$taxon_full_name,
  c(" A "," A","_"," X","\\*","×"," \\? ")," x ",fixed=F)
# make sure the author is separated in taxon_full_name
all_data2$taxon_full_name <- gsub("\\("," (",all_data2$taxon_full_name)
all_data2$taxon_full_name <- str_squish(all_data2$taxon_full_name)
unique(all_data2$taxon_full_name)

# add space after periods in taxon_full_name
all_data2$taxon_full_name <- gsub(".",". ",all_data2$taxon_full_name,fixed=T)
all_data2$taxon_full_name <- str_squish(all_data2$taxon_full_name)

# summary of genera for each institution
gen_summary <- all_data2 %>%
  arrange(genus) %>%
  rename(genera = genus) %>%
  group_by(inst_short) %>%
  mutate(
    genera = paste(unique(genera), collapse = '; ')) %>%
  ungroup() %>%
  distinct(inst_short,genera)
head(as.data.frame(gen_summary),n=30)
nrow(gen_summary) # number of institutions -- 438
# write file
write.csv(gen_summary, file.path(exsitu_dir,data_out,
                              paste0("Genera_Institutions_Summary_", 
                                     Sys.Date(), ".csv")),row.names = F)

################################################################################
# 5. Further standardize taxon name, then keep data for target taxa only
################################################################################

# fixing some taxon name issues I noticed:
all_data2$taxon_full_name <- mgsub(all_data2$taxon_full_name,
  c("Corylus Acolurnoides","Prunus Ayedoensis","Pyrus (Malus) fusca"),
  c("Corylus x colurnoides","Prunus x yedoensis","Malus fusca"))
all_data2$taxon_full_name <- gsub("Prunus virginiata","Prunus virginiana",all_data2$taxon_full_name)
all_data2$taxon_full_name <- gsub("A "," ",all_data2$taxon_full_name)

## first change hybrid notation temporarily: remove space so it stays together
all_data2$taxon_full_name <- gsub(" x "," +",all_data2$taxon_full_name)

# separate out taxon full name and trim whitespace again
    # this warning is ok: "Expected 9 pieces. Additional pieces discarded..."
all_data2 <- all_data2 %>% separate("taxon_full_name",
  c("genus_new","species_new","extra1","extra2",
    "extra3","extra4","extra5","extra6","extra7"),sep=" ",extra="warn",
    remove=F,fill="right")
all_data2 <- as.data.frame(lapply(all_data2,str_squish),stringsAsFactors=F)
# replace genus_new with genus, since we fixed that up in the previous section
all_data2$genus_new <- all_data2$genus

## REMOVE RECORDS WITHOUT SPECIES NAME

# remove records with no/non-standard specific epithet (mostly cultivars or
#   'sp.' or questionable '?') by looking in species name column
all_data3 <- all_data2 %>%
  filter(!grepl("\"",species_new) &
                !grepl("\'",species_new) &
                !grepl("\\[",species_new) &
                !grepl("\\(",species_new) &
                !grepl("\\.",species_new) &
                !grepl("[A-Z]",species_new) &
                !grepl("[0-9]",species_new) &
                !grepl("\\?",species_new) &
                !is.na(species_new))
nrow(all_data3) #150598
# see records removed; can add anything you want to fix to the
#   "fixing some taxon name issues I noticed" section above:
sort(unique(anti_join(all_data2,all_data3)$taxon_full_name))

## FIND INFRATAXA

## look for infrataxa key words
# make data in all "extra" columns lower case
sp_col <- grep("^species_new$", colnames(all_data3))
all_data3[,sp_col:(sp_col+5)] <- as.data.frame(sapply(
  all_data3[,sp_col:(sp_col+5)], tolower), stringsAsFactors=F)
# create matrix of all "extra" species name columns, to search for
#   infraspecific key words
search.col <- matrix(cbind(all_data3$extra1,all_data3$extra2,all_data3$extra3,
  all_data3$extra4,all_data3$extra5,all_data3$extra6,all_data3$extra7),
  nrow=nrow(all_data3))
#str(search.col)
# search the "extra" column matrix for matches to infraspecific key words
matches_i <- which(search.col=="variety"|search.col=="var"|search.col=="var."|
                  search.col=="v"|search.col=="v."|search.col=="va"|
                 search.col=="subspecies"|search.col=="subsp"|
                  search.col=="subsp."|search.col=="ssp"|search.col=="ssp."|
                  search.col=="subs."|search.col=="spp."|search.col=="sub."|
                 search.col=="infra"|
                 search.col=="forma"|search.col=="form"|search.col=="fma"|
                  search.col=="fo"|search.col=="fo."|search.col=="f"|
                  search.col=="f.",arr.ind=T)
matches_i[,2] <- matches_i[,2]+sp_col
# create new infra_rank column and fill with "extra" contents that matched
#   infraspecific key words
all_data3$infra_rank_new <- NA
all_data3$infra_rank_new[matches_i] <- all_data3[matches_i]
#unique(all_data3$infra_rank_new) # check results

# create new infra_name column and fill with next column over from "extra"
#   contents that matched infraspecific key word
all_data3$infra_name_new <- NA
matches_i[,2] <- matches_i[,2]+1
all_data3$infra_name_new[matches_i] <- all_data3[matches_i]
#sort(unique(all_data3$infra_name_new))

# standardize infraspecific rank names
all_data3$infra_rank_new <- replace(all_data3$infra_rank_new,
  grep("^v$|^v.$|^var$|^variety$|^va$",all_data3$infra_rank_new), "var.")
all_data3$infra_rank_new <- replace(all_data3$infra_rank_new,
  grep("^subspecies$|^subsp$|^ssp$|^ssp.$|^subs.$|^spp.$|^sub.$",
  all_data3$infra_rank_new), "subsp.")
all_data3$infra_rank_new <- replace(all_data3$infra_rank_new,
 grep("^forma$|^form$|^fma$|^fo$|^fo.$|^f$",all_data3$infra_rank_new), "f.")
unique(all_data3$infra_rank_new)

## CREATE FINAL TAXON FULL NAME FOR FILTERING

# create new taxon full name column
all_data3$taxon_full_name <- NA
  # select rows with infraspecific name and concatenate
yes_infra <- which(!is.na(all_data3$infra_rank_new) &
  !is.na(all_data3$infra_name_new))
all_data3$taxon_full_name[yes_infra] <- paste(all_data3$genus_new[yes_infra],
  all_data3$species_new[yes_infra], all_data3$infra_rank_new[yes_infra],
  all_data3$infra_name_new[yes_infra],sep=" ")
  # select rows without infraspecific name and concatenate
all_data3$taxon_full_name[-yes_infra] <- paste(all_data3$genus_new[-yes_infra],
  all_data3$species_new[-yes_infra],sep=" ")
# switch hybrid symbol back to " x "
all_data3$taxon_full_name <- gsub(" \\+"," x ",all_data3$taxon_full_name)
sort(unique(all_data3$taxon_full_name))

## FILTER OUT NON-TARGET TAXA

# rename some taxon name columns to preserve originals
all_data3 <- all_data3 %>%
  rename(taxon_name = taxon_full_name,
                genus_orig = genus,
                species_orig = species,
                infra_rank_orig = infra_rank,
                infra_name_orig = infra_name)
all_data3 <- all_data3 %>%
  rename(genus = genus_new,
                species = species_new,
                infra_rank = infra_rank_new,
                infra_name = infra_name_new)

# join dataset to taxa list
### ROUND 1: match full taxon name
  # remove genus column in taxa list if you have it
taxon_list <- taxon_list %>% select(-genus)
  # add genus_species column to taxon list
taxon_list$genus_species <- NA
taxon_list$genus_species <- sapply(taxon_list$taxon_name, function(x)
  unlist(strsplit(x," var. | subsp. | f. "))[1])
  # join by taxon name
all_data4 <- full_join(all_data3,taxon_list)
table(all_data4$taxon_name_status) # Accepted: 10627 | Synonym: 577 
### ROUND 2: switch subsp to var & var to subsp
need_match1 <- all_data4[which(is.na(all_data4$taxon_name_status)),]
nrow(need_match1) #139565
  # switch subsp and var
need_match1$taxon_name <- gsub(" subsp. "," var ",need_match1$taxon_name)
need_match1$taxon_name <- gsub(" var. "," subsp ",need_match1$taxon_name)
need_match1$taxon_name <- mgsub(need_match1$taxon_name,
                              c(" var "," subsp "),c(" var. "," subsp. "),
                              fixed=T)
sort(unique(need_match1$taxon_name))
  # remove columns from first taxon name match
need_match1 <- need_match1[,1:(ncol(all_data4)-ncol(taxon_list)+1)]
  # new join
need_match1 <- left_join(need_match1,taxon_list)
table(need_match1$taxon_name_status) # Accepted: 1 | Synonym: 0
### ROUND 3: match just by species name
need_match2 <- need_match1[which(is.na(need_match1$taxon_name_status)),]
nrow(need_match2) #139564
    # remove columns from first taxon name match
need_match2 <- need_match2[,1:(ncol(all_data4)-ncol(taxon_list)+1)]
    # remove taxon_name col from taxon data so it doesn't match
taxon_list_sp <- taxon_list %>% 
  arrange(taxon_name_status) %>%
  distinct(genus_species,.keep_all=T) %>%
  select(-taxon_name)
  # create genus_species column
need_match2$genus_species <- paste(need_match2$genus,need_match2$species)
    # new join by genus_species
need_match2 <- left_join(need_match2,taxon_list_sp); nrow(need_match2)
  # bind together new matches and previously matched
matched1 <- all_data4[which(!is.na(all_data4$taxon_name_status)),]
matched2 <- need_match1[which(!is.na(need_match1$taxon_name_status)),]
matched <- rbind(matched1,matched2)
all_data4 <- rbind(matched,need_match2)
  table(all_data4$taxon_name_status) # Accepted: 10943 | Synonym: 580
  # see how many rows have taxon name match
nrow(all_data4[which(!is.na(all_data4$taxon_name_status)),]) #11523
#table(all_data4$taxon_region)
head(all_data4)
# fill in extra data for synonyms
all_data4 <- all_data4 %>% 
  select(-iucnredlist_category,-natureserve_rank,-fruit_nut)
taxon_list_add <- taxon_list %>% 
  filter(taxon_name_status == "Accepted") %>%
  select(taxon_name_accepted,iucnredlist_category,natureserve_rank,fruit_nut)
all_data4 <- left_join(all_data4,taxon_list_add)

### CHECK UNMATCHED SPECIES, TO ADD TO SYNONYM LIST AS NECESSARY ###
check <- all_data4 %>% filter(is.na(taxon_name_status))
check <- data.frame(taxon_name = sort(unique(check$taxon_name)))
head(check); nrow(check) #576
check
# write file for checking, as desired
  # IF YOU FIND MISSPELLINGS AND/OR ADDITIONAL SYNONYMS, YOU CAN ADD THEM TO
  #   YOUR TARGET TAXA LIST AND GO BACK TO THE LINE WHERE WE "read in target
  #   taxa list" (SECTION 4) AND RUN AGAIN FROM THERE
write.csv(check, file.path(exsitu_dir,data_out,"ExSitu_UnmatchedSpecies.csv"),
  row.names = F)

# keep only matched names
all_data5 <- all_data4 %>% 
  filter(!is.na(taxon_name_status) & !is.na(inst_short))
nrow(all_data5) #11405
# see target taxa with no data
unique(taxon_list$taxon_name_accepted)[
  !(unique(taxon_list$taxon_name_accepted) %in% 
      (unique(all_data5$taxon_name_accepted)))]
# "Asimina incana"                     "Asimina manasota"                   
# "Asimina pygmaea"                    "Asimina x nashii"                  
# "Carya x lecontei"                   "Carya x ludoviciana"               
# "Juglans major var. major"           "Juglans microcarpa var. microcarpa" 
# "Juglans microcarpa var. stewartii"  "Prunus gracilis" 
# "Prunus murrayana"                   "Prunus x orthosepala" 

# see names with an x in the full name but not the accepted name
unique(all_data5[which(grepl(" x ",all_data5$taxon_full_name_orig) &
                         !grepl(" x ",all_data5$taxon_name_accepted)),
                 c("taxon_full_name_orig","taxon_name_accepted")])
# these are hybrids we don't want; remove
all_data5 <- all_data5 %>%
  filter(!(grepl(" x ",all_data5$taxon_full_name_orig) &
           !grepl(" x ",all_data5$taxon_name_accepted)))
nrow(all_data5) #11361

# final part for removing duplicate data - from previous years or networks
### UPDATE THIS SECTION BASED ON YOUR SPECIFIC NEEDS; THIS IS FOR QUERCUS 2022:
  # for 2021 and 2022 data, only remove older data for species provided
all_data6 <- all_data5
#dup_2022 <- unique(all_data6[which(all_data6$submission_year=="2022"),
#  c("taxon_name","inst_short")]); dup_2022
#for(i in 1:nrow(dup_2022)){
#  all_data6 <- all_data6 %>%
#   filter(!(taxon_name == dup_2022[i,1] &
#            inst_short == dup_2022[i,2] &
#            submission_year != "2022"))
#}; nrow(all_data6) #46666
#dup_2021 <- unique(all_data6[which(all_data6$submission_year=="2021"),
#  c("taxon_name","inst_short")]); dup_2021
#for(i in 1:nrow(dup_2021)){
#  all_data6 <- all_data6 %>%
#  filter(!(taxon_name == dup_2021[i,1] &
#           inst_short == dup_2021[i,2] &
#           submission_year != "2021"))
#}; nrow(all_data6) #46623 (was 10831 before)

################################################################################
# 6. Standardize important columns
################################################################################

## KEEP ONLY NECESSARY COLUMNS
keep_col <- c(
  # key data
  "UID","inst_short","taxon_name_accepted","acc_num","prov_type","num_indiv",
  # locality
  "orig_lat","orig_long","locality","municipality","county","state","country",
  "gps_det","uncertainty","assoc_sp",
  # source
  "orig_source","lin_num","coll_name","coll_num","coll_year",
  # material info
  "germ_type","garden_loc","rec_as","rec_date",
  # other metadata
  "notes","filename","submission_year","data_source",#"dataset_year",
  # taxon name details
  "taxon_full_name_orig","taxon_full_name_concat",
  "genus","hybrid","species","infra_rank","infra_name","cultivar",
  "taxon_name_status","author","taxon_verif",
  # OPTIONAL additional taxon metadata
  "iucnredlist_category","natureserve_rank","fruit_nut"#,"taxon_region"
)
all_data7 <- all_data6[,keep_col]

# add institution metadata to data
str(inst_data)
  # SELECT COLUMNS YOU WANT ADDED TO ALL DATA:
inst_data <- inst_data %>%
  select(inst_short,inst_country,inst_lat,inst_long,inst_type) %>%
  arrange(inst_lat) %>%
  distinct(inst_short,.keep_all=T) %>%
  arrange(inst_short); inst_data
all_data7 <- left_join(all_data7,inst_data)
str(all_data7)

##
## Provenance type
##

# look at column contents and change below phrases as needed
all_data7$orig_prov_type <- all_data7$prov_type
all_data7$prov_type <- str_to_lower(all_data7$prov_type)
sort(unique(all_data7$prov_type))
## IF NEEDED: transfer contents of one column to another column, if data
#   needs to be preserved but is in the wrong place
# Warnings are fine here
all_data7[which(all_data7$prov_type=="130"|grepl("^130) ",all_data7$prov_type)),]$notes <- "Semi-natural/sown"
#all_data7[which(all_data7$prov_type=="410"|grepl("^410) ",all_data7$prov_type)),]$notes <- "Breeding/research material: Breeder's line"
all_data7[which(all_data7$prov_type=="300"|grepl("^300) ",all_data7$prov_type)),]$notes <- "Traditional cultivar/landrace"
all_data7[which(all_data7$prov_type=="400"|grepl("^400) ",all_data7$prov_type)),]$notes <- "Breeding/research material"
all_data7[which(all_data7$prov_type=="500"|grepl("^500) ",all_data7$prov_type)),]$notes <- "Advanced or improved cultivar (conventional breeding methods)"
all_data7[which(grepl("county",all_data7$prov_type)),]$notes <- all_data7[which(grepl("county",all_data7$prov_type)),]$prov_type

# standardize column by searching for keywords and replacing with standard value
  # remove confusing words/phrases
all_data7$prov_type <- mgsub(all_data7$prov_type,
  c("not of known wild origin","could be cultivated","not of wild source",
    "wildsourceunsure","cultivated plant of known "), "")
  # ex wild (Z)
all_data7$prov_type <- ifelse(grepl(paste(
  c("indirect","ex wild","^z$","cultivated from wild","c ex w","cw",
    "g from w plant","g ex w","cultivatedpropagatedfromwildsource",
    "(indirect) wild origin"),
  collapse = "|"), all_data7$prov_type),"Z",all_data7$prov_type)
  # wild (W)
all_data7$prov_type <- ifelse(grepl(paste(
  c("wild","wld","collect","^w$","^w\\*","^\\(w\\)$","wd","w\\?","genetic",
    "100","110","130"),
  collapse = "|"), all_data7$prov_type),"W",all_data7$prov_type)
  # native to site (N)
all_data7$prov_type <- ifelse(grepl(paste(
  c("original to site","spontaneous","^n$","existing on site","native"),
  collapse = "|"), all_data7$prov_type),"N",all_data7$prov_type)
  # unknown (U)
all_data7$prov_type <- ifelse(grepl(paste(
  c("^\\(u\\)$","^u$","^u ","unsure","insufficient data","unknown","\\?","un",
    "need to confirm","breeding","400","410","donated","999","purchased"),
  collapse = "|"), all_data7$prov_type),"U",all_data7$prov_type)
  # cultivated (H)
all_data7$prov_type <- ifelse(grepl(paste(
  c("cultiva","garden","^c$","^g$","^g ","^h$","horticult","landrace","clone",
    "300","500","^g\\."),
  collapse = "|"), all_data7$prov_type),"H",all_data7$prov_type)
# check one last time
sort(unique(all_data7$prov_type))
  # not given (NG) ; everything else
all_data7$prov_type <- ifelse(all_data7$prov_type!= "W" &
  all_data7$prov_type != "Z" & all_data7$prov_type != "H" &
  all_data7$prov_type != "N" & all_data7$prov_type != "U",
  "NG",all_data7$prov_type)
all_data7$prov_type[which(is.na(all_data7$prov_type))] <- "NG"

# check results
table(all_data7$prov_type)
#    H    N   NG    U    W    Z
# 2856   25 2816 1641 3841  182 

##
## B) Number of Individuals
##

all_data7$num_indiv <- str_to_lower(all_data7$num_indiv)
sort(unique(all_data7$num_indiv))
  ## IF NEEDED: replace unwanted characters
  all_data7$num_indiv <- mgsub(all_data7$num_indiv,
    c(" \\(all dead\\)"," \\(removed 2017\\)","\\?",","," plants"," or 5",
      " pieces","ca\\. ","ca\\."," alive","\\*","1 \\(","\\)","\\+",
      " in terra","alive;","a a a a a a a a a a a",";[0-9][0-9]",";1",
      ";2",";3",";4",";5",";7",";0",";"),
      c(""), fixed=F)
  sort(unique(all_data7$num_indiv))
  # change type to numeric and replace NA with 1
  # "NAs introduced by coercion" warning ok
all_data7$num_indiv <- as.numeric(all_data7$num_indiv)
all_data7$num_indiv[which(is.na(all_data7$num_indiv))] <- 1

# check results
sort(unique(all_data7$num_indiv))
nrow(all_data7) #11361

# remove records with no individuals; save as separate file
no_indiv <- all_data7[which(all_data7$num_indiv == 0),]
nrow(no_indiv) #918
write.csv(no_indiv, file.path(exsitu_dir,data_out,
  paste0("ExSitu_Dead_", Sys.Date(), ".csv")),row.names = F)
  # save to in situ data folder as well
if(!dir.exists(file.path(main_dir,"occurrence_data","raw_occurrence_data","Ex-situ")))
  dir.create(file.path(main_dir,"occurrence_data","raw_occurrence_data","Ex-situ"),
  recursive=T)
write.csv(no_indiv, file.path(main_dir,"occurrence_data","raw_occurrence_data","Ex-situ",
  paste0("ExSitu_Dead_", Sys.Date(), ".csv")),row.names = F)
all_data7 <- all_data7[which(all_data7$num_indiv > 0),]
nrow(all_data7) #10443

##
## C) Latitude and Longitude
##

all_data8 <- all_data7
all_data8$temp_id <- seq.int(nrow(all_data8))

# preserve original lat and long columns
all_data8$lat_dd <- all_data8$orig_lat
all_data8$long_dd <- all_data8$orig_long

# fix some common lat/long character issues
all_data8$lat_dd <- mgsub(all_data8$lat_dd,
  c("\"","ç","d","'","°","º","Â","¼","،","¡","_","´","*","À","?","`")," ")
all_data8$long_dd <- mgsub(all_data8$long_dd,
  c("\"","ç","d","'","°","º","Â","¼","،","¡","_","´","*","À","?","`")," ")
# replace comma with decimal (european notation)
all_data8$lat_dd <- mgsub(all_data8$lat_dd, c(","), ".")
all_data8$long_dd <- mgsub(all_data8$long_dd, c(","), ".")
# separate values if lat and long both ended up in the lat column
all_data8[which(grepl("\\. ",all_data8$lat_dd)),] <-
  separate(all_data8[which(grepl("\\. ",all_data8$lat_dd)),], col = lat_dd,
           into = c("lat_dd","long_dd"), sep = "\\. ", remove = FALSE)

# replace unwanted characters
  ## latitude
  # replace random unnecessary characters
all_data8$lat_dd <- mgsub(all_data8$lat_dd,
  c("N","\\","/","M","A",": ","E","AZ","R","d","a"," \\."," W")," ")
    # remove leading zero
all_data8$lat_dd[which(grepl("^ *[0][1-9]+",all_data8$lat_dd))] <- gsub(
  "^ *[0]","",all_data8$lat_dd[which(grepl("^ *[0][1-9]+",all_data8$lat_dd))])
all_data8$lat_dd[which(grepl("^S *[0][1-9]+",all_data8$lat_dd))] <- gsub(
  "^S *[0]","-",all_data8$lat_dd[which(grepl("^S *[0][1-9]+",all_data8$lat_dd))])
    # add negative sign if south and remove "S"
all_data8$lat_dd[grep("S",all_data8$lat_dd,ignore.case=T)] <-
  paste("-",all_data8$lat_dd[grep("S",all_data8$lat_dd,ignore.case=T)],sep="")
all_data8$lat_dd <- gsub("S","",all_data8$lat_dd)
all_data8$lat_dd <- gsub("--","-",all_data8$lat_dd)
    # remove double spaces or leading/trailing whitespace
all_data8$lat_dd <- str_squish(all_data8$lat_dd)
sort(unique(all_data8$lat_dd))
  # can check source of specific values that aren't formatted correctly
#all_data8[which(all_data8$lat_dd == "422538"),]
  ## longitude
all_data8$long_dd <- replace_non_ascii(all_data8$long_dd,
  replacement=" ", remove.nonconverted=T)
all_data8$long_dd <- mgsub(all_data8$long_dd,
  c("E","\\","/","NR","d","A","a"," .","o","O")," ")
all_data8$long_dd[which(grepl("^ *[0][1-9]+",all_data8$long_dd))] <- gsub(
  "^ *[0]","",all_data8$long_dd[which(grepl("^ *[0][1-9]+",all_data8$long_dd))])
all_data8$long_dd[which(grepl("^W *[0][1-9]+",all_data8$long_dd))] <- gsub(
  "^W *[0]","-",all_data8$long_dd[which(grepl("^W *[0][1-9]+",
    all_data8$long_dd))])
all_data8$long_dd[grep("W",all_data8$long_dd,ignore.case=T)] <-
  paste("-",all_data8$long_dd[grep("W",all_data8$long_dd,ignore.case=T)],sep="")
all_data8$long_dd <- gsub("W","",all_data8$long_dd)
all_data8$long_dd <- mgsub(all_data8$long_dd,c("--","- "),"-")
all_data8$long_dd <- str_squish(all_data8$long_dd)
sort(unique(all_data8$long_dd))
  # USBG gave data with species name in lat-long fields :(

# convert decimal-minutes-seconds (dms) to decimal degrees (dd)
#   [d, m, and s must be in the same cell, with 1 space between each value]
#   format = ## ## ## (DMS) OR ## ##.### (DM)
  # mark rows that need to be converted
convert <- all_data8[which(grepl(" ",all_data8$lat_dd) |
  grepl(" ",all_data8$long_dd)),]
  nrow(convert) #132
unique(convert$lat_dd)
good <- anti_join(all_data8, convert)
  # separate by dec_min_sec and deg_dec_min then convert to decimal degrees
    # latitude
dms <- convert[which(str_count(convert$lat_dd," ") == 2),]; nrow(dms)
ddm <- convert[which(str_count(convert$lat_dd," ") == 1),]; nrow(ddm)
other <- convert[which((str_count(convert$lat_dd," ") != 1 &
  str_count(convert$lat_dd," ") != 2) | is.na(str_count(convert$lat_dd," "))),]
  nrow(other)
dms$lat_dd = measurements::conv_unit(dms$lat_dd, from = 'deg_min_sec',
  to = 'dec_deg')
ddm$lat_dd = measurements::conv_unit(ddm$lat_dd, from = 'deg_dec_min',
  to = 'dec_deg')
convert <- rbind(dms,ddm,other); nrow(convert)
    # longitude
dms <- convert[which(str_count(convert$long_dd," ") == 2),]; nrow(dms)
ddm <- convert[which(str_count(convert$long_dd," ") == 1),]; nrow(ddm)
other <- convert[which((str_count(convert$long_dd," ") != 1 &
  str_count(convert$long_dd," ") != 2) | is.na(str_count(convert$long_dd," "))),]
  nrow(other)
  dms$long_dd = measurements::conv_unit(dms$long_dd, from = 'deg_min_sec',
    to = 'dec_deg')
  ddm$long_dd = measurements::conv_unit(ddm$long_dd, from = 'deg_dec_min',
    to = 'dec_deg')
  convert <- rbind(dms,ddm,other); nrow(convert)
  # join everything back together
all_data8 <- rbind(good,convert); nrow(all_data8)

# check validity of lat and long
all_data8$lat_dd <- as.numeric(all_data8$lat_dd)
  #sort(unique(all_data8$lat_dd))
all_data8$long_dd <- as.numeric(all_data8$long_dd)
  #sort(unique(all_data8$long_dd))
  # if coords are both 0, set to NA
zero <- which(all_data8$lat_dd == 0 & all_data8$long_dd == 0)
all_data8$lat_dd[zero] <- NA; all_data8$long_dd[zero] <- NA
  # flag non-numeric and not available coordinates and lat > 90, lat < -90,
  # lon > 180, and lon < -180
coord_test <- cc_val(all_data8, lon = "long_dd",lat = "lat_dd",
  value = "flagged", verbose = TRUE) #Flagged 8709 records.
  # try switching lat and long for invalid points and check validity again
all_data8[!coord_test,c("lat_dd","long_dd")] <-
  all_data8[!coord_test,c("long_dd","lat_dd")]
coord_test <- cc_val(all_data8,lon = "long_dd",lat = "lat_dd",
  value = "flagged",verbose = TRUE) #Flagged 8709 records.

# check longitude values that are positive, since
# sometimes the negative gets left off
pos_long <- all_data8[which(all_data8$long_dd > 0),]
# IF the taxon is native to N/S America, make long value negative:
#unique(all_data8$taxon_region)
#setDT(all_data8)[long_dd > 0 & taxon_region != "Europe" &
#                 taxon_region != "Asia" & taxon_region != "Asia & Europe",
#                 long_dd := as.numeric(paste0("-",as.character(long_dd)))]
setDT(all_data8)[long_dd > 0,
                 long_dd := as.numeric(paste0("-",as.character(long_dd)))]

# make coords NA if they are still flagged
coord_test <- cc_val(all_data8,lon = "long_dd",lat = "lat_dd",
  value = "flagged",verbose = TRUE)
all_data8[!coord_test,"lat_dd"] <- NA
all_data8[!coord_test,"long_dd"] <- NA
nrow(all_data8) #10443

### now we'll work just with the geolocated points for a bit
all_data8$flag <- ""
have_coord <- all_data8 %>% filter(!is.na(lat_dd) & !is.na(long_dd))
nrow(have_coord) #1734
no_coord <- anti_join(all_data8,have_coord)
no_coord$latlong_country <- ""
# add country-level information to fix potentially-switched lat/longs
  # turn occurrence point data into a spatial object
crdref <- "+proj=longlat +datum=WGS84"
geo_pts_spatial <- vect(cbind(have_coord$long_dd,have_coord$lat_dd),
  atts=have_coord, crs=crdref)
world_polygons <- vect(file.path(main_dir,"gis_layers",
  "UIA_World_Countries_Boundaries/World_Countries__Generalized_.shp"))
# add country polygon data to each point based on lat-long location
geo_pts <- terra::intersect(geo_pts_spatial,world_polygons)
# try switching lat and long for points in Antarctica
on_land <- as.data.frame(geo_pts); nrow(on_land) #1619
on_land[which(on_land$COUNTRY == "Antarctica"),c("lat_dd","long_dd")] <-
  on_land[which(on_land$COUNTRY == "Antarctica"),c("long_dd","lat_dd")]
head(on_land)
# remove columns from first join and join to countries again
on_land <- on_land %>% select(UID:flag)
geo_pts_spatial <- vect(cbind(on_land$long_dd,on_land$lat_dd),
  atts=on_land, crs=crdref)
geo_pts <- terra::intersect(geo_pts_spatial,world_polygons)
on_land <- geo_pts %>%
  select(UID:flag,COUNTRY) %>%
  rename(latlong_country = COUNTRY)
on_land <- as.data.frame(on_land)
nrow(on_land) #1587
land_id <- unique(on_land$temp_id)
# check if geolocated points are in water and mark
  # get points that have coords but didn't fall in a country;
  # these are in the water
in_water <- have_coord %>% filter(!(temp_id %in% land_id))
in_water <- as.data.frame(in_water)
in_water$flag <- "Given lat-long is in water"
in_water$latlong_country <- ""
nrow(in_water) #147
# bind all the points back together
all_data9 <- rbind(on_land,in_water,no_coord)
nrow(all_data9)

# mark lat-long for records with same inst lat-long and wild lat-long
all_data9$lat_round <- round(all_data9$lat_dd,digits=1)
all_data9$long_round <- round(all_data9$long_dd,digits=1)
all_data9$inst_lat_round <- round(all_data9$inst_lat,digits=1)
all_data9$inst_long_round <- round(all_data9$inst_long,digits=1)
garden_latlong <- all_data9 %>% filter(lat_round == inst_lat_round &
  long_round == inst_long_round & prov_type != "N")
unique(garden_latlong$inst_short)
nrow(garden_latlong)
all_data9[which(all_data9$temp_id %in% garden_latlong$temp_id),]$flag <-
  "Given lat-long is at institution, use only if native to grounds"
#all_data9[all_data9$UID %in% garden_latlong$UID,]$lat_dd <- NA
#all_data9[all_data9$UID %in% garden_latlong$UID,]$long_dd <- NA
table(all_data9$flag) #grounds = 154; water = 147

# add gps_det (gps determination) column
all_data9$gps_det[which(all_data9$prov_type == "H")] <- "N/A (horticultural)"
all_data9$gps_det[which(!is.na(all_data9$lat_dd) &
  !is.na(all_data9$long_dd))] <- "Given"
all_data9$gps_det[which(all_data9$gps_det == "")] <- NA
table(all_data9$gps_det)
# Given     N/A (horticultural)
# 1734      2660 

# where prov_type is "N/A (horticultural)" but lat-long is given, change to "H?"
  # create new prov type column
all_data9$prov_type[which(all_data9$gps_det == "Given" &
  all_data9$prov_type == "H")] <- "H?"
table(all_data9$prov_type)
#    H   H?    N   NG    U    W    Z
# 2660  121   25 2277 1586 3598  176

##
## D) Collection year
##

# not using this part right now
sort(unique(all_data9$coll_year))
# separate additional years
#all_data9 <- all_data9 %>% 
#  separate("coll_year","coll_year",sep="; ",remove=F)
#all_data9 <- all_data9 %>% 
#  separate("coll_year","coll_year",sep=";",remove=F)

## IF NEEDED: replace non-year words/characters
#all_data9$coll_year <- mgsub(all_data9$coll_year,
#  c("----","about ","ca.","Unknown","original","Estate","estate","<",
#    "NEAR ","PRE "),"")
#all_data9$coll_year[which(all_data9$coll_year == "")] <- NA
#all_data9$coll_year <- gsub("-","/",all_data9$coll_year)
#all_data9$coll_year <- gsub("\\.","/",all_data9$coll_year)
#sort(unique(all_data9$coll_year))

# remove extra elements so its just year
#all_data9$coll_year <- gsub("^[0-9][0-9]/[0-9][0-9]/","",all_data9$coll_year)
#all_data9$coll_year <- gsub("^[0-9]/[0-9][0-9]/","",all_data9$coll_year)
#all_data9$coll_year <- gsub("^[0-9][0-9]/[0-9]/","",all_data9$coll_year)
#all_data9$coll_year <- gsub("^[0-9]/[0-9]/","",all_data9$coll_year)
#all_data9$coll_year <- gsub("^[0-9][0-9]/[A-Z][a-z][a-z]/","",all_data9$coll_year)
#all_data9$coll_year <- gsub("^[0-9]/[A-Z][a-z][a-z]/","",all_data9$coll_year)
#all_data9$coll_year <- gsub("^[0-9][0-9]/","",all_data9$coll_year)
#all_data9$coll_year <- gsub("/[0-9][0-9]$","",all_data9$coll_year)
#all_data9$coll_year <- gsub("^[0-9][0-9] ","",all_data9$coll_year)
#all_data9$coll_year <- gsub("^[0-9] ","",all_data9$coll_year)

# make column numeric
#all_data9$coll_year <- as.numeric(all_data9$coll_year)
## IF NEEDED: add first two numbers in year
#  # assume 2000s if values is less than 21
#all_data9$coll_year[which(all_data9$coll_year < 10)] <-
#  paste0("200",as.character(all_data9$coll_year[which(all_data9$coll_year < 10)]))
#all_data9$coll_year <- as.numeric(all_data9$coll_year)
#all_data9$coll_year[which(all_data9$coll_year < 21)] <-
#  paste0("20",as.character(all_data9$coll_year[which(all_data9$coll_year < 21)]))
#all_data9$coll_year <- as.numeric(all_data9$coll_year)
#  # assume 1900s if values is greater than or equal to 21
#all_data9$coll_year[which(all_data9$coll_year < 100)] <-
#  paste0("19",as.character(all_data9$coll_year[which(all_data9$coll_year < 100)]))
#all_data9$coll_year <- as.numeric(all_data9$coll_year)
#sort(unique(all_data9$coll_year))

##
## E) Lineage number
##

# remove lin_num when same as acc_num
all_data9[which(all_data9$acc_num == all_data9$lin_num),]$lin_num <- NA

##
## F) Locality
##

# create all_locality column
all_data9$latitude <- round(all_data9$lat_dd,digits=4)
all_data9$longitude <- round(all_data9$long_dd,digits=4)
all_data9 <- unite(all_data9, "all_locality",
  c(locality,municipality,county,state,country,orig_source,
    lin_num,coll_num,coll_name,coll_year,
    latitude,longitude,notes),sep = " | ",remove = F)
# remove NA in concatenated locality column
all_data9$all_locality <- gsub("NA","",all_data9$all_locality)
# if no locality info at all, make it NA
all_data9$all_locality[which(all_data9$all_locality ==
  " |  |  |  |  |  |  |  |  |  |  |  | ")] <- NA

##
## G) Institution type
##

# add inst_type for gene bank data (didn't do this for Quercus)
all_data9$inst_type[which(is.na(all_data9$inst_type))] <- "Gene/Seed Bank"
table(all_data9$inst_type)
# Botanic Garden    Botanic Garden; Gene/Seed Bank     Gene/Seed Bank
# 7843              151                                2449
table(all_data9$data_source)
# ex_situ_BG_survey  FAO-WIEWS   Genesys
# 9151               1108        184

##
## H) Combine individuals (same institution and accession number)
##      so that everything is (hopefully) at the accession-level
##

# save version without duplicates combined, in cases needed for reference
write.csv(all_data9, file.path(exsitu_dir,data_out,
                              paste0("ExSitu_Compiled_No-Dups-Combined_", 
                                     Sys.Date(), ".csv")),row.names = F)

# preserve original acc_num before removing individual signifiers
all_data9$orig_acc_num <- all_data9$acc_num
# fill any spaces in acc_num with dashes
all_data9$acc_num <- gsub(" ","-",all_data9$acc_num)

# combine duplicates (same acc num)
all_data9 <- all_data9 %>%
  group_by(inst_short,acc_num,taxon_name_accepted) %>%
  mutate(num_indiv = sum(as.numeric(num_indiv)),
         germ_type = paste(unique(germ_type),collapse="; "),
         garden_loc = paste(unique(garden_loc),collapse="; "),
         orig_acc_num = paste(unique(orig_acc_num),collapse="; "),
         all_locality = paste(unique(all_locality),collapse="; ")) %>%
  ungroup() %>%
  distinct(inst_short,acc_num,taxon_name_accepted,.keep_all=T)
nrow(all_data9) #9381

# now we will try to find patterns that identify accession numbers
# with individual identifiers - we want everything to be at the
# acccession level, not at the individual level for this analysis

# function to remove individual-specific identifiers (to combine dup accessions)
# "Expected 1 pieces" warning here is fine
combine_acc_dups <- function(df,char_cutoff,pattern){
  # create subset of records with acc_num longer than 9 characters
  #   (these are usually the ones with plant identifiers; some are missed
  #    but this gets most of them)
  check_accnum <- df[which(nchar(df$acc_num)>char_cutoff),]
  print(nrow(check_accnum))
  no_check_accnum <- setdiff(df,check_accnum)
  print(nrow(no_check_accnum))
  # combine duplicates using the pattern provided to cut off characters
  check_accnum <- check_accnum %>%
    separate("acc_num","acc_num",sep=pattern,remove=F) %>%
    group_by(inst_short,acc_num,taxon_name_accepted) %>%
    mutate(num_indiv = sum(as.numeric(num_indiv)),
           germ_type = paste(unique(germ_type),collapse="; "),
           garden_loc = paste(unique(garden_loc),collapse="; "),
           orig_acc_num = paste(unique(orig_acc_num),collapse="; "),
           all_locality = paste(unique(all_locality),collapse="; ")) %>%
    ungroup() %>%
    distinct(inst_short,acc_num,taxon_name_accepted,.keep_all=T)
  # create and return full data frame of checked and unchecked rows
  df_return <- full_join(check_accnum,no_check_accnum)
  return(df_return)
}

### THE NEXT SECTION NEEDS TO BE THOROUGHLY REVEIWED AND 
###  EDITED TO FIT YOUR DATASET !
### Otherwise you will combine records that do not belong together

# can look at what will be combined before we actually combine things...
all_data9[which(grepl("\\*",all_data9$acc_num)),]$acc_num
# now combine...
all_data9 <- combine_acc_dups(all_data9,0,"\\*")
nrow(all_data9) #8527
# now we'll repeat for additional patterns and cutoffs
sort(all_data9[which(grepl("[A-F]$",all_data9$acc_num)),]$acc_num)
  all_data9 <- combine_acc_dups(all_data9,9,"[A-F]$"); nrow(all_data9)
sort(all_data9[which(grepl("-[1-9]$",all_data9$acc_num)),]$acc_num)
  all_data9 <- combine_acc_dups(all_data9,9,"-[1-9]$"); nrow(all_data9)
sort(all_data9[which(grepl("-[0-9][0-9]$",all_data9$acc_num)),]$acc_num)
  all_data9 <- combine_acc_dups(all_data9,9,"-[0-9][0-9]$"); nrow(all_data9)
sort(all_data9[which(grepl("-[0][0][0-9]$",all_data9$acc_num)),]$acc_num)
 all_data9 <- combine_acc_dups(all_data9,9,"-[0][0][0-9]$"); nrow(all_data9)
sort(all_data9[which(grepl("/[0-9]$",all_data9$acc_num)),]$acc_num)
 all_data9 <- combine_acc_dups(all_data9,0,"/[0-9]$"); nrow(all_data9)
sort(all_data9[which(grepl("/[0-9][0-9]$",all_data9$acc_num)),]$acc_num)
  all_data9 <- combine_acc_dups(all_data9,9,"/[0-9][0-9]$"); nrow(all_data9)
sort(all_data9[which(grepl("\\.[0-9]$",all_data9$acc_num)),]$acc_num)
  all_data9 <- combine_acc_dups(all_data9,0,"\\.[0-9]$"); nrow(all_data9)
sort(all_data9[which(grepl("\\.[0-9][0-9]$",all_data9$acc_num)),]$acc_num)
  all_data9 <- combine_acc_dups(all_data9,9,"\\.[0-9][0-9]$"); nrow(all_data9)
sort(all_data9[which(grepl("\\.[0-9][0-9][0-9]$",all_data9$acc_num)),]$acc_num)
  all_data9 <- combine_acc_dups(all_data9,9,"\\.[0-9][0-9][0-9]$"); nrow(all_data9)
# can also check which records contain specific elements from above patterns:
#as.data.frame(all_data7[which(grepl("-DCH-18A;-TF-6A",all_data7$acc_num)),])
#as.data.frame(all_data7[which(all_data7$acc_num == "1964-0017-00"),])

# remove ending separation characters 
all_data9$acc_num <- mgsub(all_data9$acc_num,
                              c("/$","-$","\\.$"),"",fixed=F)
unique(all_data9$acc_num)

nrow(all_data9) #8384

##
## I) Add Unique ID Column
##

# add unique ID column
# create UID with institution name, acc num, provenance type, and taxon name
# also remove duplicates based on new UID and sum individuals
# now create UID and remove dups
need_id <- all_data9[which(is.na(all_data9$UID)),]
dont_need_id <- all_data9[which(!is.na(all_data9$UID)),]
nms <- names(need_id)
nrow(need_id)
need_id <- need_id %>%
  arrange(orig_lat,locality) %>%
  mutate(UID = paste(inst_short,acc_num,prov_type,taxon_name_accepted,sep="~")) %>%
  group_by(UID) %>%
  mutate(num_indiv = sum(as.numeric(num_indiv))) %>%
  distinct(UID,.keep_all=T) %>%
  ungroup() %>%
  select(c("UID",all_of(nms)))
all_data9 <- rbind(need_id,dont_need_id)
nrow(all_data9) #8380

################################################################################
# 7. Summary statistics
################################################################################

## FINAL VERSION: KEEP ONLY NECESSARY COLUMNS
keep_col <- c(
  # key data
  "UID","inst_short","taxon_name_accepted","acc_num","prov_type","num_indiv",
  # locality
  "lat_dd","long_dd","flag","gps_det","uncertainty","all_locality",
  "locality","municipality","county","state","country",
  "latlong_country","assoc_sp",
  # source
  "orig_source","lin_num","coll_name","coll_num","coll_year",
  # material info
  "germ_type","garden_loc","rec_as",
  # other metadata
  "notes","filename","submission_year","data_source",#"dataset_year",
  # taxon name details
  "taxon_full_name_orig","taxon_full_name_concat",
  "genus","hybrid","species","infra_rank","infra_name","cultivar",
  "taxon_name_status","author","taxon_verif",
  # institution metadata
  "inst_country","inst_lat","inst_long","inst_type",
  # original versions of columns, for reference
  "orig_prov_type","orig_acc_num","orig_lat","orig_long",
  # OPTIONAL additional taxon metadata
  "iucnredlist_category","natureserve_rank","fruit_nut"#,"taxon_region"
)
data_sel <- all_data9[,keep_col]

# write file
write.csv(data_sel, file.path(exsitu_dir,data_out,
  paste0("All_ExSitu_Compiled_", Sys.Date(), ".csv")),row.names = F)

# [OPTIONAL] regional subsets for easier use:
#meso <- data_sel %>% filter(grepl("Mesoamerica",taxon_region))
#nrow(meso) #6514
#write.csv(meso, file.path(exsitu_dir,data_out,
#  paste0("Mesoamerican-spp_ExSitu_Compiled_", Sys.Date(), ".csv")),
#  row.names = F)


################################################################################
# 8. (Optionally) Explore geoerferencing needs
################################################################################

### explore georeferencing needs
# table with...
#   species name
#   num wild acc
#   num non-H acc w/ coords
#   num non-H acc with no coords & yes locality info
geo_needs <- data_sel %>%
  group_by(taxon_name_accepted) %>%
  summarize(
    num_acc = sum(!is.na(taxon_name_accepted)),
    num_wild = sum(prov_type == "W"),
    NotH_YesCoords = sum(!is.na(lat_dd) & prov_type != "H"),
    NotH_NoCoords_YesLocality = sum(is.na(lat_dd) & 
                                      !is.na(all_locality) & 
                                      prov_type != "H"),
    Percent_NonH_NeedGeo = (sum(is.na(lat_dd) & 
                                  !is.na(all_locality) & 
                                  prov_type != "H") 
                            / sum(prov_type != "H")*100)
  )
head(geo_needs,n=20)
# write file
write.csv(geo_needs, file.path(exsitu_dir,data_out,
  paste0("ExSitu_Geolocation_Needs_Summary_", Sys.Date(), ".csv")),
  row.names = F)

# records that may need geolocation
#   (no lat-long, yes locality, prov type not H)
#   (also add flagged records: water or at institution)
need_geo <- data_sel %>%
  filter((is.na(lat_dd) & prov_type != "H" &
            !is.na(all_locality) & all_locality != "NA") |
         flag!="")
nrow(need_geo) #4506
# add a couple more columns for keeping notes while geolocating
need_geo$geolocated_by <- NA
need_geo$gps_notes <- NA
# condense all_locality duplicates
need_geo <- need_geo %>%
  group_by(prov_type,lat_dd,long_dd,gps_det,all_locality) %>%
  mutate(UID = paste0(UID,collapse=" | "),
         inst_short = paste0(unique(inst_short),collapse=" | "),
         taxon_name_accepted = paste0(unique(taxon_name_accepted),
                                      collapse=" | ")) %>%
  ungroup() %>%
  distinct(UID,inst_short,taxon_name_accepted,prov_type,lat_dd,long_dd,
           uncertainty,gps_det,geolocated_by,gps_notes,all_locality,county,
           state,country,flag) %>%
  select(UID,inst_short,taxon_name_accepted,prov_type,lat_dd,long_dd,
         uncertainty,gps_det,geolocated_by,gps_notes,all_locality,county,
         state,country,flag)
nrow(need_geo) #2907
head(need_geo)
# flag records for species that are high priority...
#   threatened and/or have less than 15 wild accessions
#   (you can choose whatever threshold you want)
  # thresholds
rl_threat <- c("CR (Critically Endangered)","EN (Endangered)","VU (Vulnerable)")
ns_threat <- c("G1 (Critically Imperiled)","G2 (Imperiled)","G3 (Vulerable)")
few_wild <- geo_needs[geo_needs$num_wild<15,]$taxon_name_accepted
  # flag priority taxa
priority_taxa <- taxon_list %>%
  filter(iucnredlist_category %in% rl_threat |
         natureserve_rank %in% ns_threat |
         taxon_name %in% few_wild) %>%
  select(taxon_name_accepted)
priority_taxa$priority <- "Priority"
need_geo <- left_join(need_geo,priority_taxa)
table(need_geo$priority) #456

# write file
write.csv(need_geo, file.path(exsitu_dir,data_out,
  paste0("ExSitu_Need_Geolocation_", Sys.Date(), ".csv")),row.names = F)

### NOW MANUALLY GEOLOCATE !
### INSTRUCTIONS FOR GEOLOCATING:
### https://docs.google.com/document/d/1RBUD6-ogLc7PRVkDJSIEKzkgvC6xekj9Q_kl1vzxhCs/edit?usp=sharing

################################################################################
# 9. Add geolocated data, after manual geolocation
################################################################################

# read in all compiled ex situ data (exported above)
exsitu <- read.csv(file.path(exsitu_dir,data_out,
  "All_ExSitu_Compiled_2022-12-07.csv"), header = T, colClasses="character")

# read in geolocated dataset
geo_raw <- read.csv(file.path(exsitu_dir,data_out,
  "ExSitu_Need_Geolocation_2022-12-07_Geolocated.csv"),
  header = T, colClasses="character")
head(geo_raw)
  # check this is just NA and no "priority" records that are not geolocated
unique(geo_raw[which(is.na(geo_raw$gps_det)),"priority"])

# add geolocated coordinates to ex situ data
  # separate UID row
geolocated <- separate_rows(geo_raw, UID, sep=" \\| ")
#OLD: geolocated <- separate_rows(geo_raw, UID, sep="\\|;\\|")
#OLD: geolocated$UID <- gsub("UCalifornia BGerkeley","UCaliforniaBGBerkeley",geolocated$UID)
  # keep only edited rows (lat, long, gps_det) and
  #   records that have gps_det filled in
geolocated <- geolocated %>%
  select(UID,lat_dd,long_dd,gps_det,uncertainty,geolocated_by,gps_notes,
         county,state) %>%
  filter(!is.na(gps_det))
head(geolocated)
table(geolocated$gps_det)
      #   L     C    G    X
      #   289   62   259   1331
  # select geolocated rows in full dataset and remove cols we want to add
exsitu_geo <- exsitu %>%
  filter(UID %in% geolocated$UID) %>%
  select(-lat_dd,-long_dd,-gps_det,-uncertainty,-county,-state)
    # these two values should be the same:
nrow(exsitu_geo) #1941
nrow(geolocated) #1941
  # add geolocation data
exsitu_geo <- full_join(exsitu_geo,geolocated)
  # join geolocated rows with rest of ex situ rows
exsitu_no_geo <- exsitu %>%
  filter(!(UID %in% exsitu_geo$UID))
nrow(exsitu_no_geo)
exsitu_all <- rbind.fill(exsitu_no_geo,exsitu_geo)
nrow(exsitu_all)
table(exsitu_all$gps_det)
      #   L     C    G     X     H
      #   289   62   1261  1331  2095

# write new file
write.csv(exsitu_all, file.path(exsitu_dir,data_out,
  paste0("All_ExSitu_Compiled_Post-Geolocation_", Sys.Date(), ".csv")), 
  row.names = F)
# write to in situ folder also
write.csv(exsitu_all, file.path(main_dir,"occurrence_data",
                                "raw_occurrence_data","Ex-situ",
  paste0("ExSitu_Compiled_Post-Geolocation_", Sys.Date(), ".csv")), 
  row.names = F)











### THIS SECTION IS OLD AND HAS NOT BEEN UPDATED !!!
### it goes between step 8 and 9 above, if using

################################################################################
# 8.5 (Optionally) Create file(s) for GEOLocate
################################################################################

# read in data in again, if you didn't just run the whole script
#need_geo <- read.csv(file.path(main_dir,"ex-situ_data","OUTPUTS_FROM_R",
#  "ExSitu_Need_Geolocation_2022-10-14.csv"), header = T, colClasses="character")

# add GEOLocate standard columns
need_geo$correction.status <- NA
need_geo$precision <- NA
need_geo$error.polygon <- NA
need_geo$multiple.results <- NA
need_geo$latitude <- NA
need_geo$longitude <- NA

# update column name and order for GEOLocate requirements
geolocate <- need_geo %>%
  # rename to GEOLocate standard columns
  rename(locality.string = all_locality) %>%
  # order records alphabetically
  arrange(taxon_name_acc,locality.string) %>%
  # reorder columns
  select(
    ## GeoLocate
    locality.string,country,state,county,latitude,longitude,
    correction.status,precision,error.polygon,multiple.results,uncertainty,
    ## metadata to include also
    taxon_name_acc,inst_short,prov_type,gps_det,geolocated_by,gps_notes,UID) %>%
  # replace NA with "" to make simpler to view in GEOLocate
  replace(., is.na(.), "")
head(geolocate)

# remove dot in column names (replace with space) for GEOLocate
names(geolocate) <- gsub(x = names(geolocate),pattern = "\\.",replacement = " ")
str(geolocate)
head(as.data.frame(geolocate))

# split by species, so each species has separate file;
#   this step makes it easier to use the GEOLocate tool because it doesn't
#   handle lots of records very well

# create one CSV for each target species
# !!! this doesn't work well if you combined species above to remove locality duplicates !!!
sp_split <- split(geolocate, as.factor(geolocate$taxon_name_acc))
names(sp_split) <- gsub(" ","_",names(sp_split))

# write files
if(!dir.exists(file.path(main_dir,"ex-situ_data","OUTPUTS_FROM_R","files_for_GEOLocate")))
  dir.create(file.path(main_dir,"ex-situ_data","OUTPUTS_FROM_R","files_for_GEOLocate"),recursive=T)
lapply(seq_along(sp_split), function(i) write.csv(sp_split[[i]],
  file.path(main_dir,"ex-situ_data","OUTPUTS_FROM_R","files_for_GEOLocate",
  paste0(names(sp_split)[[i]],".csv")),row.names = F))


### !!!
### ! NOW GO GEOLOCATE !
### !!!



# Other random old code chunk:

### TESTING GEONAMES PACKAGE ###

# read in compiled ex situ data
df <- read.csv(file.path(main_dir,"outputs",
  "exsitu_compiled_standardized_2021-02-11_firstpassGpsDet.csv"),
  header = T, colClasses="character")
str(df)
unique(df$gps_det)
tail(df[which(df$gps_det == "L"),])

# create login: https://www.geonames.org/login
# https://www.geonames.org/manageaccount
#   the account is not yet enabled to use the free web services. Click here to enable.
usethis::edit_r_environ()
devtools::install_github("ropensci/geonames")
library(geonames)

# https://rstudio-pubs-static.s3.amazonaws.com/489236_0259d8532a354ad6945d818bc4c052f1.html

login <- read_lines(log_loc)
username  <- login[4]
options(geonamesUsername = username)
head(GNsearch(name = "Chicago"))
#  adminCode1       lng geonameId               toponymName countryId fcl population countryCode                      name           fclName adminCodes1.ISO3166_2   countryName                                      fcodeName adminName1      lat fcode
#1         IL -87.65005   4887398                   Chicago   6252001   P    2720546          US                   Chicago city, village,...                    IL United States seat of a second-order administrative division   Illinois 41.85003 PPLA2
#2         IL -87.89062   6955104 Chicago-Naperville-Joliet   6252001   L    9569624          US Chicago-Naperville-Joliet   parks,area, ...                    IL United States                                economic region   Illinois 41.70778  RGNE
#3         IL  -87.6979  12070033 Chicago metropolitan area   6252001   L    9472676          US Chicago metropolitan area   parks,area, ...                    IL United States                                         region   Illinois  41.8902   RGN
#4         OH -82.72629   5176830                   Willard   6252001   P       6063          US                   Willard city, village,...                    OH United States                                populated place       Ohio 41.05311   PPL
#5         IL -87.69644   4887463              Chicago Lawn   6252001   P      55551          US              Chicago Lawn city, village,...                    IL United States                     section of populated place   Illinois 41.77503  PPLX
#6         IL -87.55425   4911863             South Chicago   6252001   P      28095          US             South Chicago city, village,...                    IL United States                                populated place   Illinois 41.73977   PPL

lanc_coords <- lanc_df[1, c("lng", "lat")]
